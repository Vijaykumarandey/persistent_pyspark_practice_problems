{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d65fa2e1-b969-4ce9-8078-098ad7e46da2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>name</th><th>region</th><th>status</th><th>signup_date</th></tr></thead><tbody><tr><td>1</td><td>Alice Johnson</td><td>West</td><td>Active</td><td>2023-01-15</td></tr><tr><td>2</td><td>Bob Smith</td><td>East</td><td>Inactive</td><td>2023-02-20</td></tr><tr><td>3</td><td>Charlie Lee</td><td>West</td><td>Active</td><td>2023-03-10</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Alice Johnson",
         "West",
         "Active",
         "2023-01-15"
        ],
        [
         2,
         "Bob Smith",
         "East",
         "Inactive",
         "2023-02-20"
        ],
        [
         3,
         "Charlie Lee",
         "West",
         "Active",
         "2023-03-10"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "region",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "signup_date",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[customer_id: int, name: string, region: string]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "from datetime import datetime\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CustomerData\").getOrCreate()\n",
    "\n",
    "# Stub for creating the schema in Unity Catalog (assume catalog 'retail' exists)\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS retail.customer_data\")\n",
    "\n",
    "# Define schema for customer data\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"signup_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Sample input data as a list of rows\n",
    "customer_data = [\n",
    "    (1, \"Alice Johnson\", \"West\", \"Active\",datetime(2023,1,15)),\n",
    "    (2, \"Bob Smith\", \"East\", \"Inactive\",datetime(2023,2,20)),\n",
    "    (3, \"Charlie Lee\", \"West\", \"Active\", datetime(2023,3,10))\n",
    "]\n",
    "\n",
    "# Create DataFrame from sample data\n",
    "customers_df = spark.createDataFrame(customer_data, schema=customer_schema)\n",
    "display(customers_df)\n",
    "customers_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"retail.customer_data.customers\")\n",
    "spark.sql(\"SELECT customer_id, name, region FROM retail.customer_data.customers WHERE status = 'Active' AND region = 'West'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f23182e4-bde9-445f-8dd8-f893800027c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"InventoryUpdates\").getOrCreate()\n",
    "\n",
    "# Stub for creating the schema in Unity Catalog (assume catalog 'ecommerce' exists)\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS ecommerce.inventory\")\n",
    "\n",
    "# Define schema for inventory\n",
    "inventory_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"stock\", IntegerType(), True),\n",
    "    StructField(\"warehouse\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Initial inventory data\n",
    "initial_data = [\n",
    "    (101, \"Laptop\", 50, \"A\"),\n",
    "    (102, \"Smartphone\", 100, \"B\")\n",
    "]\n",
    "initial_df = spark.createDataFrame(initial_data, schema=inventory_schema)\n",
    "#display(initial_df)\n",
    "#Write initial data as Delta table\n",
    "initial_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"ecommerce.inventory.products\")\n",
    "\n",
    "# Update batch data\n",
    "update_data = [\n",
    "    (101, \"Laptop\", 45, \"A\"),\n",
    "    (103, \"Tablet\", 30, \"A\")\n",
    "]\n",
    "update_df = spark.createDataFrame(update_data, schema=inventory_schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "247cb7d9-2ac7-425a-9dfc-d07247a5ab15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+---------+\n|product_id|      name|stock|warehouse|\n+----------+----------+-----+---------+\n|       101|    Laptop|   45|        A|\n|       102|Smartphone|  100|        B|\n|       103|    Tablet|   30|        A|\n+----------+----------+-----+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Get reference to the Delta table\n",
    "delta_inventory = DeltaTable.forName(spark, \"ecommerce.inventory.products\")\n",
    "\n",
    "# Merge (upsert) update_df into the target table\n",
    "delta_inventory.alias(\"target\").merge(\n",
    "    update_df.alias(\"source\"),\n",
    "    \"target.product_id = source.product_id\"\n",
    ").whenMatchedUpdate(\n",
    "    set={\"stock\": \"source.stock\"}\n",
    ").whenNotMatchedInsert(\n",
    "    values={\n",
    "        \"product_id\": \"source.product_id\",\n",
    "        \"name\": \"source.name\",\n",
    "        \"stock\": \"source.stock\",\n",
    "        \"warehouse\": \"source.warehouse\"\n",
    "    }\n",
    ").execute()\n",
    "\n",
    "# Query the final inventory\n",
    "final_df = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM ecommerce.inventory.products\n",
    "    ORDER BY product_id\n",
    "\"\"\")\n",
    "final_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "342d4b5a-6a04-486f-a89c-0532274fceac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------+--------------------+--------------------+--------------------+----+--------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|version|           timestamp|        userId|            userName|           operation| operationParameters| job|notebook|           clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+--------------------+--------------+--------------------+--------------------+--------------------+----+--------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|     11| 2025-08-12 10:00:48|70887064104782|vijaykumarsbsp@gm...|               MERGE|{predicate -> [\"(...|NULL|    NULL|0812-091116-xn91f...|         10|WriteSerializable|        false|{numTargetRowsCop...|        NULL|Databricks-Runtim...|\n|     10| 2025-08-12 10:00:45|70887064104782|vijaykumarsbsp@gm...|CREATE OR REPLACE...|{partitionBy -> [...|NULL|    NULL|0812-091116-xn91f...|          8|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n|      9| 2025-08-12 10:00:44|70887064104782|vijaykumarsbsp@gm...|            OPTIMIZE|{clusterBy -> [],...|NULL|    NULL|0812-091116-xn91f...|          8|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|      8| 2025-08-12 10:00:42|70887064104782|vijaykumarsbsp@gm...|               MERGE|{predicate -> [\"(...|NULL|    NULL|0812-091116-xn91f...|          7|WriteSerializable|        false|{numTargetRowsCop...|        NULL|Databricks-Runtim...|\n|      7| 2025-08-12 10:00:39|70887064104782|vijaykumarsbsp@gm...|CREATE OR REPLACE...|{partitionBy -> [...|NULL|    NULL|0812-091116-xn91f...|          6|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n|      6|2025-08-12 09:58:...|70887064104782|vijaykumarsbsp@gm...|CREATE OR REPLACE...|{partitionBy -> [...|NULL|    NULL|0812-091116-xn91f...|          4|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n|      5| 2025-08-12 09:58:33|70887064104782|vijaykumarsbsp@gm...|            OPTIMIZE|{clusterBy -> [],...|NULL|    NULL|0812-091116-xn91f...|          4|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|      4| 2025-08-12 09:58:31|70887064104782|vijaykumarsbsp@gm...|               MERGE|{predicate -> [\"(...|NULL|    NULL|0812-091116-xn91f...|          3|WriteSerializable|        false|{numTargetRowsCop...|        NULL|Databricks-Runtim...|\n|      3| 2025-08-12 09:58:28|70887064104782|vijaykumarsbsp@gm...|CREATE OR REPLACE...|{partitionBy -> [...|NULL|    NULL|0812-091116-xn91f...|          2|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n|      2| 2025-08-12 09:57:06|70887064104782|vijaykumarsbsp@gm...|            OPTIMIZE|{clusterBy -> [],...|NULL|    NULL|0812-091116-xn91f...|          1|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|      1| 2025-08-12 09:57:05|70887064104782|vijaykumarsbsp@gm...|               MERGE|{predicate -> [\"(...|NULL|    NULL|0812-091116-xn91f...|          0|WriteSerializable|        false|{numTargetRowsCop...|        NULL|Databricks-Runtim...|\n|      0| 2025-08-12 09:57:02|70887064104782|vijaykumarsbsp@gm...|CREATE OR REPLACE...|{partitionBy -> [...|NULL|    NULL|0812-091116-xn91f...|       NULL|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n+-------+--------------------+--------------+--------------------+--------------------+--------------------+----+--------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n\n+------------+\n|total_amount|\n+------------+\n|       27000|\n+------------+\n\n+------------+\n|total_amount|\n+------------+\n|       27000|\n+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SalesAnalysis\").getOrCreate()\n",
    "\n",
    "# Stub for creating the schema in Unity Catalog (assume catalog 'finance' exists)\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS finance.sales\")\n",
    "\n",
    "# Define schema for sales\n",
    "sales_schema = StructType([\n",
    "    StructField(\"sale_id\", IntegerType(), False),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True),\n",
    "    StructField(\"quarter\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Initial sales data\n",
    "initial_data = [\n",
    "    (1, \"Stocks\", 10000, \"Q1-2023\"),\n",
    "    (2, \"Bonds\", 15000, \"Q1-2023\")\n",
    "]\n",
    "initial_df = spark.createDataFrame(initial_data, schema=sales_schema)\n",
    "\n",
    "# Write initial data as Delta table with history enabled\n",
    "initial_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"finance.sales.quarterly_sales\")\n",
    "\n",
    "# Update data (correction)\n",
    "update_data = [\n",
    "    (1, \"Stocks\", 12000, \"Q1-2023\")\n",
    "]\n",
    "update_df = spark.createDataFrame(update_data, schema=sales_schema)\n",
    "\n",
    "# Perform update\n",
    "delta_table = DeltaTable.forName(spark, \"finance.sales.quarterly_sales\")\n",
    "delta_table.alias(\"target\").merge(\n",
    "    update_df.alias(\"source\"),\n",
    "    \"target.sale_id = source.sale_id\"\n",
    ").whenMatchedUpdate(set={\"amount\": \"source.amount\"}).execute()\n",
    "\n",
    "# Create schema if not exists\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS finance.sales\")\n",
    "\n",
    "# Initial sales data\n",
    "initial_df.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"finance.sales.quarterly_sales\")\n",
    "\n",
    "# Update data\n",
    "update_df = spark.createDataFrame(\n",
    "    [(1, \"Stocks\", 12000, \"Q1-2023\")],\n",
    "    schema=sales_schema\n",
    ")\n",
    "\n",
    "delta_sales = DeltaTable.forName(spark, \"finance.sales.quarterly_sales\")\n",
    "delta_sales.alias(\"target\").merge(\n",
    "    update_df.alias(\"source\"),\n",
    "    \"target.sale_id = source.sale_id\"\n",
    ").whenMatchedUpdate(\n",
    "    set={\"amount\": \"source.amount\"}\n",
    ").execute()\n",
    "\n",
    "# History\n",
    "history_df = delta_sales.history()\n",
    "history_df.show()\n",
    "\n",
    "# Previous version\n",
    "previous_version = history_df.select(\"version\").collect()[1][0]\n",
    "prev_total_df = spark.sql(f\"\"\"\n",
    "    SELECT SUM(amount) AS total_amount\n",
    "    FROM finance.sales.quarterly_sales VERSION AS OF {previous_version}\n",
    "\"\"\")\n",
    "prev_total_df.show()\n",
    "\n",
    "# Current version\n",
    "spark.sql(\"\"\"\n",
    "    SELECT SUM(amount) AS total_amount\n",
    "    FROM finance.sales.quarterly_sales\n",
    "\"\"\").show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "practice_problems",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}